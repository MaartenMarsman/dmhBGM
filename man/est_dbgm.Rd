% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/est_dbgm.R
\name{est_dbgm}
\alias{est_dbgm}
\title{Bayesian estimation of a Markov Random Field of mixed binary and ordinal
variables using Double Metropolis-Hastings.}
\usage{
est_dbgm(
  x,
  iter = 10000,
  burnin = 1000,
  dmhsamples = 1,
  cauchy_scale = 2.5,
  threshold_alpha = 0.5,
  threshold_beta = 0.5,
  save = FALSE,
  display_progress = TRUE,
  parallel = FALSE
)
}
\arguments{
\item{x}{A data frame or matrix with \code{n} rows and \code{p} columns
containing binary and ordinal variables for \code{n} independent observations
and \code{p} variables in the network. Variables are recoded as non-negative
integers \code{(0, 1, ..., m)} if not already done. Unobserved categories are
collapsed into other categories after recoding (i.e., if category 1 is
unobserved, the data will be recoded from (0, 2) to (0, 1)).}

\item{iter}{The number of iterations of the Gibbs sampler. The default of
\code{1e4} is for illustrative purposes. For stable estimates, it is
recommended to run the Gibbs sampler for at least \code{1e5} iterations.}

\item{burnin}{The number of iterations of the Gibbs sampler before its output
is saved. Since it may take some time for the Gibbs sampler to converge to
the posterior distribution, it is recommended not to set this number too low.}

\item{dmhsamples}{The DMH approach generates a new, augmented data set to
update model parameters with Metropolis Hastings. It must generate a new
augmented data set for each parameter of the model in each iteration of the
Gibbs sampler. Since we cannot sample these data directly from the MRF, we
must use a Gibbs sampler to generate them.  The dmhsamples argument controls how
many iterations are used for this "inner" Gibbs sampler. The default of
\code{1} is for illustrative purposes. For stable estimates,
\insertCite{ParkEtAl_2020}{dmhBGM} recommend running the DMH-Gibbs sampler
for at least \code{n * p} iterations.}

\item{cauchy_scale}{The scale of the Cauchy prior for interactions. Defaults
to \code{2.5}.}

\item{threshold_alpha, threshold_beta}{The shape parameters of the beta-prime
prior density for the threshold parameters. Must be positive values. If the
two values are equal, the prior density is symmetric about zero. If
\code{threshold_beta} is greater than \code{threshold_alpha}, the
distribution is skewed to the left, and if \code{threshold_beta} is less than
\code{threshold_alpha}, it is skewed to the right. Smaller values tend to
lead to more diffuse prior distributions.}

\item{save}{Should the function collect and return all samples from the Gibbs
sampler (\code{save = TRUE})? Or should it only return the (model-averaged)
posterior means (\code{save = FALSE})? Defaults to \code{FALSE}.}

\item{display_progress}{Should the function show a progress bar
(\code{display_progress = TRUE})? Or not (\code{display_progress = FALSE})?
Defaults to \code{TRUE}.}

\item{parallel}{Should data generation for the DMH algorithtm be parallelized?
Defaults to \code{FALSE}.}
}
\value{
If \code{save = FALSE} (the default), the result is a list of class
``dmhBGM'' containing the following matrices:
\itemize{
\item \code{interactions}: A matrix with \code{p} rows and \code{p} columns,
containing model-averaged posterior means of the pairwise associations.
\item \code{thresholds}: A matrix with \code{p} rows and \code{max(m)}
columns, containing model-averaged category thresholds.
}

If \code{save = TRUE}, the result is a list of class ``dmhBGM'' containing:
\itemize{
\item \code{interactions}: A matrix with \code{iter} rows and
\code{p * (p - 1) / 2} columns, containing parameter states from every
iteration of the Gibbs sampler for the pairwise associations.
\item \code{thresholds}: A matrix with \code{iter} rows and
\code{sum(m)} columns, containing parameter states from every iteration of
the Gibbs sampler for the category thresholds.
}
Column averages of these matrices provide the model-averaged posterior means.
}
\description{
The function \code{est_dbgm} explores the joint posterior distribution
of the parameters of a Markov Random Field for mixed binary and ordinal
variables. It uses a combination of Metropolis-Hastings and Gibbs sampling to
create a Markov chain that has the posterior distribution as its invariant
distribution. This package uses Double Metropolis Hastings
\insertCite{Liang_2010}{dmhBGM} to circumvent having to compute the
intractable normalizing constant of the Markov Random Field, and uses an
adaptive Metropolis to "learn" an optimal proposal distribution: Adjusting
the proposal variance to match the acceptance probability of the
random walk Metropolis algorithm to be close to the optimum of \code{.234}
using a Robbins-Monro type algorithm.
}
\details{
The prior distribution for the interactions is a Cauchy. A Beta-prime
distribution is used for the exponent of the category parameters.
}
\references{
\insertAllCited{}
}
