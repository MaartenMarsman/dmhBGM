% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/dbgm.R
\name{dbgm}
\alias{dbgm}
\title{Bayesian structure learning in Markov Random Fields of mixed binary and
ordinal variables using MCMC.}
\usage{
dbgm(
  x,
  iter = 10000,
  burnin = 1000,
  dmhsamples = 1,
  cauchy_scale = 2.5,
  edge_prior = c("Bernoulli", "Beta-Bernoulli"),
  inclusion_probability = 0.5,
  beta_bernoulli_alpha = 1,
  beta_bernoulli_beta = 1,
  threshold_alpha = 0.5,
  threshold_beta = 0.5,
  save = FALSE,
  display_progress = TRUE,
  parallel = FALSE,
  no_cores = RcppParallel::defaultNumThreads() - 1
)
}
\arguments{
\item{x}{A data frame or matrix with \code{n} rows and \code{p} columns
containing binary and ordinal variables for \code{n} independent observations
and \code{p} variables in the network. Variables are recoded as non-negative
integers \code{(0, 1, ..., m)} if not already done. Unobserved categories are
collapsed into other categories after recoding (i.e., if category 1 is
unobserved, the data will be recoded from (0, 2) to (0, 1)).}

\item{iter}{The number of iterations of the Gibbs sampler. The default of
\code{1e4} is for illustrative purposes. For stable estimates, it is
recommended to run the Gibbs sampler for at least \code{1e5} iterations.}

\item{burnin}{The number of iterations of the Gibbs sampler before its output
is saved. Since it may take some time for the Gibbs sampler to converge to
the posterior distribution, it is recommended not to set this number too low.}

\item{dmhsamples}{The DMH approach generates a new, augmented data set to
update model parameters with Metropolis Hastings. It must generate a new
augmented data set for each parameter of the model in each iteration of the
Gibbs sampler. Since we cannot sample these data directly from the MRF, we
must use a Gibbs sampler to generate them.  The dmhsamples argument controls how
many iterations are used for this "inner" Gibbs sampler. The default of
\code{1} is for illustrative purposes. For stable estimates,
\insertCite{ParkEtAl_2020}{dmhBGM} recommend running the DMH-Gibbs sampler
for at least \code{n * p} iterations.}

\item{cauchy_scale}{The scale of the Cauchy prior for interactions. Defaults
to \code{2.5}.}

\item{edge_prior}{The prior distribution for the edges or structure of the
network. Two prior distributions are currently implemented: The Bernoulli
model \code{edge_prior = "Bernoulli"} assumes that the probability that an
edge between two variables is included is equal to
\code{inclusion_probability} and independent of other edges or variables.
When \code{inclusion_probability = 0.5}, this implies that each network
structure receives the same prior weight. The Beta-Bernoulli model
\code{edge_prior = "Beta-Bernoulli"} assumes a beta prior for the unknown
inclusion probability with shape parameters \code{beta_bernoulli_alpha} and
\code{beta_bernoulli_beta}. If \code{beta_bernoulli_alpha = 1} and
\code{beta_bernoulli_beta = 1}, this means that networks with the same
complexity (number of edges) receive the same prior weight. Defaults to
\code{edge_prior = "Bernoulli"}.}

\item{inclusion_probability}{The prior edge inclusion probability for the
Bernoulli model. Can be a single probability, or a matrix of \code{p} rows
and \code{p} columns specifying an inclusion probability for each edge pair.
Defaults to \code{inclusion_probability = 0.5}.}

\item{beta_bernoulli_alpha, beta_bernoulli_beta}{The two shape parameters of
the Beta prior density for the Bernoulli inclusion probability. Must be
positive numbers. Defaults to \code{beta_bernoulli_alpha = 1} and
\code{beta_bernoulli_beta = 1}.}

\item{threshold_alpha, threshold_beta}{The shape parameters of the beta-prime
prior density for the threshold parameters. Must be positive values. If the
two values are equal, the prior density is symmetric about zero. If
\code{threshold_beta} is greater than \code{threshold_alpha}, the
distribution is skewed to the left, and if \code{threshold_beta} is less than
\code{threshold_alpha}, it is skewed to the right. Smaller values tend to
lead to more diffuse prior distributions.}

\item{save}{Should the function collect and return all samples from the Gibbs
sampler (\code{save = TRUE})? Or should it only return the (model-averaged)
posterior means (\code{save = FALSE})? Defaults to \code{FALSE}.}

\item{display_progress}{Should the function show a progress bar
(\code{display_progress = TRUE})? Or not (\code{display_progress = FALSE})?
Defaults to \code{TRUE}.}

\item{parallel}{Should the DMH iterations be run in parallel? Defaults to \code{FALSE}.}

\item{no_cores}{If the iterations for DMH are run in parallel, how many cores should be used?
Defaults to \code{RcppParallel::defaultNumThreads() - 1}.}
}
\value{
If \code{save = FALSE} (the default), the result is a list of class
``dmhBGM'' containing the following matrices:
\itemize{
\item \code{gamma}: A matrix with \code{p} rows and \code{p} columns,
containing posterior inclusion probabilities of individual edges.
\item \code{interactions}: A matrix with \code{p} rows and \code{p} columns,
containing model-averaged posterior means of the pairwise associations.
\item \code{thresholds}: A matrix with \code{p} rows and \code{max(m)}
columns, containing model-averaged category thresholds.
}

If \code{save = TRUE}, the result is a list of class ``dmhBGM'' containing:
\itemize{
\item \code{gamma}: A matrix with \code{iter} rows and
\code{p * (p - 1) / 2} columns, containing the edge inclusion indicators from
every iteration of the Gibbs sampler.
\item \code{interactions}: A matrix with \code{iter} rows and
\code{p * (p - 1) / 2} columns, containing parameter states from every
iteration of the Gibbs sampler for the pairwise associations.
\item \code{thresholds}: A matrix with \code{iter} rows and
\code{sum(m)} columns, containing parameter states from every iteration of
the Gibbs sampler for the category thresholds.
}
Column averages of these matrices provide the model-averaged posterior means.
}
\description{
The function \code{dbgm} explores the joint posterior distribution of
structures and parameters in a Markov Random Field for mixed binary and
ordinal variables.
}
\details{
A discrete spike and slab prior distribution is stipulated on the pairwise
interactions. By formulating it as a mixture of mutually singular
distributions, the function can use a combination of Metropolis-Hastings and
Gibbs sampling to create a Markov chain that has the joint posterior
distribution as invariant. This package uses Double Metropolis Hastings
\insertCite{Liang_2010}{dmhBGM} to circumvent having to compute the
intractable normalizing constant of the Markov Random Field, and uses an
adaptive Metropolis to "learn" an optimal proposal distribution: Adjusting
the proposal variance to match the acceptance probability of the
random walk Metropolis algorithm to be close to the optimum of \code{.234}
using a Robbins-Monro type algorithm.

The slab distribution is a Cauchy with an optional scaling parameter. A
Beta-prime distribution is used for the exponent of the category parameters.
Two prior distributions are implemented for edge inclusion variables (i.e.,
the prior probability that an edge is included); the Bernoulli prior and the
Beta-Bernoulli prior.
}
\references{
\insertAllCited{}
}
